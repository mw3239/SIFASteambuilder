{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.agents import CategoricalDqnAgent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import categorical_q_network\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "NOTES = 100\n",
    "MAX_SP_GAUGE = 6000\n",
    "MAX_SP_VOLTAGE = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "LOG_FILENAME = datetime.now().strftime('_%H_%M_%S_%d_%m_%Y.log')\n",
    "logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.TEAM = pd.DataFrame([{'pos': 1, 'name':\"Char_1\", 'strategy':0, 'appeal':10000},\n",
    "                            {'pos': 2, 'name':\"Char_2\", 'strategy':1, 'appeal':8000},\n",
    "                            {'pos': 3, 'name':\"Char_3\", 'strategy':2, 'appeal':5000}])\n",
    "    self.NOTES = 100\n",
    "    self.MAX_SP_GAUGE = 6000\n",
    "    self.MAX_SP_VOLTAGE = 250000\n",
    "    self.sp_skill = sum(self.TEAM['appeal'])\n",
    "    if self.sp_skill > self.MAX_SP_VOLTAGE:\n",
    "        self.sp_skill = self.MAAX_SP_VOLTAGE\n",
    "    #I'm starting with 6 actions - hit, miss, 3 swaps, and SP Skill.\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=5, name='action')\n",
    "    #Currently 19 observations are set as follows:\n",
    "    #1: current note\n",
    "    #2: max notes\n",
    "    #3: score max score set to 1 billion.\n",
    "    #4: Current strategy. 0, 1, or 2 (red, geren, blue)\n",
    "    #5-7: Can you swap to each strategy? (0 or 1 depending on current strategy)\n",
    "    #8: Strat swap cooldown (From 0 to 5)\n",
    "    #9: SP Gauge value\n",
    "    #10: SP Gauge max\n",
    "    #11: SP Skill Useable?\n",
    "    #12: SP Skill Value\n",
    "    #13-15: The appeal stats of the 3 characters.\n",
    "    #16-18: The strategies of each of the 3 characters.\n",
    "    #19: S-rank score\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,18), dtype=np.float32, minimum=[1,self.NOTES,0,0,0,0,0,0,0,3600,0,0,0,0,0,0,0,0],\n",
    "                      maximum=[self.NOTES+1,self.NOTES,1e+9,2,1,1,1,5,self.MAX_SP_GAUGE,7200,1,self.MAX_SP_VOLTAGE,1e+6,1e+6,1e+6,2,2,2], name='observation')   \n",
    "    self._episode_ended = False\n",
    "    self.improper_use_counter = 0\n",
    "    self.max_mistakes = 10\n",
    "    \n",
    "    self.reward = 0\n",
    "    \n",
    "    #Can swap red and blue, but not green at start.\n",
    "    #Row 1 goes up to observation #12\n",
    "    \n",
    "    self._state = [1,self.NOTES,0,1,1,0,1,0,0,self.MAX_SP_GAUGE,0,self.sp_skill,\n",
    "                   self.TEAM['appeal'].iloc[0],self.TEAM['appeal'].iloc[1],self.TEAM['appeal'].iloc[2],self.TEAM['strategy'].iloc[0],self.TEAM['strategy'].iloc[1],self.TEAM['strategy'].iloc[2]]\n",
    "    \n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._episode_ended = False\n",
    "    self._state = [1,self.NOTES,0,1,1,0,1,0,0,self.MAX_SP_GAUGE,0,self.sp_skill,\n",
    "                   self.TEAM['appeal'].iloc[0],self.TEAM['appeal'].iloc[1],self.TEAM['appeal'].iloc[2],self.TEAM['strategy'].iloc[0],self.TEAM['strategy'].iloc[1],self.TEAM['strategy'].iloc[2]]\n",
    "    self.improper_use_counter = 0\n",
    "    self.reward = 0\n",
    "    return ts.restart(np.array([self._state], dtype=np.float32))\n",
    "\n",
    "  def decrement_swap(self):\n",
    "    if self._state[7] > 0:\n",
    "        self._state[7] -= 1\n",
    "    return(self)\n",
    "\n",
    "  def reset_swap(self):\n",
    "    self._state[7] = 5\n",
    "    return(self)\n",
    "\n",
    "  def get_possible_appeals(self,appeal):\n",
    "    self.reward += appeal - max(self.TEAM['appeal']) #Gives a reward based on how far the appealed value was from the maximum possible.\n",
    "    \n",
    "    return(self)\n",
    "\n",
    "  def hit_note(self):\n",
    "    if self._state[0] <= self._state[1]: #If there are still notes to hit \n",
    "        if self._state[3] == self._state[15]: #If the current strategy is red\n",
    "            self._state[2] += self._state[12] #Add the appeal of the red character to the score\n",
    "            self.reward += self._state[12]\n",
    "            #reward = self.get_possible_appeals(reward,self._state[12]) #And the reward is how much you appealed\n",
    "        elif self._state[3] == self._state[16]:\n",
    "            self._state[2] += self._state[13]\n",
    "            self.reward += self._state[13]\n",
    "            #reward = self.get_possible_appeals(reward,self._state[13])\n",
    "        else:\n",
    "            self._state[2] += self._state[14]\n",
    "            self.reward += self._state[14]\n",
    "            #reward = self.get_possible_appeals(reward,self._state[14])\n",
    "    \n",
    "        self._state[0] += 1 #Increase the note count by 1.\n",
    "        self.update_sp_skill() #Add to the SP gauge\n",
    "\n",
    "        self.decrement_swap() #Decrement the counter until the next swap if it's not 0 already.\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "    \n",
    "  #This is mostly for training purposes as there's obviously no practical purpose for missing the note instead of hitting it.\n",
    "  def miss_note(self):\n",
    "    if self._state[0] <= self._state[1]: #If there are still notes to hit (or miss I guess) \n",
    "        self._state[0] += 1\n",
    "        self.decrement_swap()\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "\n",
    "  def swap_red(self):\n",
    "    #Check that you're not on red already and that the cooldown is 0.\n",
    "    if self._state[4] == 1 and self._state[7] == 0: #If you can swap red and the swap is off cooldown\n",
    "        self._state[3] = 0 #Switch current strategy to red\n",
    "    \n",
    "        #Change ability to swap to other strategies\n",
    "        self._state[4] = 0 #Can no longer swap to red.\n",
    "        self._state[5] = 1 #Can swap to green\n",
    "        self._state[6] = 1 #Can swap to blue\n",
    "        \n",
    "        self.reset_swap() #Reset swap cooldown to 5\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "\n",
    "  def swap_green(self):\n",
    "    if self._state[5] == 1 and self._state[7] == 0:\n",
    "        self._state[3] = 1\n",
    "    \n",
    "        #Change ability to swap to other strategies\n",
    "        self._state[4] = 1\n",
    "        self._state[5] = 0\n",
    "        self._state[6] = 1\n",
    "        \n",
    "        self.reset_swap()\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "\n",
    "  def swap_blue(self):\n",
    "    if self._state[6] == 1 and self._state[7] == 0:\n",
    "        self._state[3] = 2\n",
    "    \n",
    "        #Change ability to swap to other strategies\n",
    "        self._state[4] = 1\n",
    "        self._state[5] = 1\n",
    "        self._state[6] = 0\n",
    "        \n",
    "        self.reset_swap()\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "\n",
    " #Increases the reward the further the agent makes it into the song.\n",
    "  def progress_bonus(self):\n",
    "    self.reward *= (self._state[0]/self._state[1])\n",
    "    return(self)\n",
    "\n",
    "  def use_sp_skill(self):\n",
    "    if self._state[10] == 1: #If the SP Skill is useable\n",
    "        self._state[10] = 0 #Set it to no longer be useable\n",
    "        self._state[8] = 0 #Set the SP gauge value to 0\n",
    "        self._state[2] += self._state[11] #Add the SP voltage to the score\n",
    "        self.reward += self._state[11] #Reward is the voltage\n",
    "    elif self.improper_use_counter <= self.max_mistakes:\n",
    "        self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        self.improper_use_counter += 1\n",
    "    else:\n",
    "        self.reward += -20000\n",
    "        self._episode_ended = True\n",
    "    return(self)\n",
    "\n",
    "  def update_sp_skill(self):\n",
    "    if self._state[8] < self._state[9]: #If the gauge isn't already full\n",
    "        self._state[8] += 200 #Add SP to the SP gauge\n",
    "    if self._state[8] >= self._state[9]: #If adding SP would've put it over the max\n",
    "        self._state[8] = self._state[9] #Set the value to the max\n",
    "        self._state[10] = 1 #And make the SP skill usable \n",
    "    return(self)      \n",
    "\n",
    "  def sp_hold_punish(self,action):\n",
    "    if self._state[10] == 1 and action!=5: #If the SP Skill is available and is not queued to be used\n",
    "        self.reward -= 1000 #Punish the agent.\n",
    "    return(self)\n",
    "\n",
    "  def _step(self, action):\n",
    "    #If The last action ended the episode ignore the current action and start a new episode.\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "    \n",
    "    #Ends after the last note, when neither a swap or SP skill are available.\n",
    "    elif self._state[0] >= self._state[1] and self._state[7] == 0 and self._state[10] == 0:\n",
    "      self._episode_ended = True\n",
    "    \n",
    "    elif action == 0:\n",
    "        self = self.hit_note()\n",
    "    elif action == 1:\n",
    "        self = self.miss_note()\n",
    "    elif action == 2:\n",
    "        self = self.swap_red()\n",
    "    elif action == 3:\n",
    "        self = self.swap_green()\n",
    "    elif action == 4:\n",
    "        self = self.swap_blue()\n",
    "    elif action == 5:\n",
    "        self = self.use_sp_skill()\n",
    "    else:\n",
    "      raise ValueError('`action` should be an integer 0 through 5.')\n",
    "    \n",
    "    if self._episode_ended:\n",
    "      #Reward for successfully clearing all notes and having no swaps or SP skill available at the end.\n",
    "      if self._state[0] >= self._state[1] and self._state[7] > 0 and self._state[10] == 0:\n",
    "          self.reward += self._state[2] + 5e+9 #Give it a fat reward for making it to the end. Tried to pick a number that will always make passing have a higher reward than failing with a high score.\n",
    "      return ts.termination(np.array([self._state], dtype=np.float32), self.reward)\n",
    "    else:\n",
    "      return ts.transition(np.array([self._state], dtype=np.float32), reward=self.reward, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongEnvLogger(SongEnv):\n",
    "    def hit_note(self):\n",
    "        appealed_for = 0\n",
    "        if self._state[0] <= self._state[1]: #If there are still notes to hit \n",
    "            if self._state[3] == self._state[15]: #If the current strategy is red\n",
    "                self._state[2] += self._state[12] #Add the appeal of the red character to the score\n",
    "                self.reward += self._state[12] #And the reward is how much you appealed\n",
    "                appealed_for = self._state[12] \n",
    "            elif self._state[3] == self._state[16]:\n",
    "                self._state[2] += self._state[13]\n",
    "                self.reward += self._state[13]\n",
    "                appealed_for = self._state[13] \n",
    "            else:\n",
    "                self._state[2] += self._state[14]\n",
    "                self.reward += self._state[14]\n",
    "                appealed_for = self._state[14] \n",
    "                \n",
    "            self._state[0] += 1 #Increase the note count by 1.\n",
    "            self.update_sp_skill() #Add to the SP gauge\n",
    "            self.decrement_swap() #Decrement the counter until the next swap if it's not 0 already.\n",
    "            \n",
    "            logging.info([self.TEAM[self.TEAM['strategy']==self._state[3]]['name'].values[0],\" appealed for \", appealed_for,\" points. Current score: \",self._state[2],\". SP Gauge: \",self._state[8],\"/\",self._state[9],\". Note:\",self._state[0],\"/\",self._state[1]])\n",
    "            \n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to hit note after song already ended. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to hit note after song already ended. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "            \n",
    "        return(self)\n",
    "    \n",
    "    def miss_note(self):\n",
    "        if self._state[0] <= self._state[1]: #If there are still notes to hit (or miss I guess) )\n",
    "            self._state[0] += 1\n",
    "            self.decrement_swap()\n",
    "            logging.info([\"Missed note. Current score: \",self._state[2],\". SP Gauge: \",self._state[8],\"/\",self._state[9],\". Note:\",self._state[0],\"/\",self._state[1]])\n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to hit note after song already ended. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to hit note after song already ended. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "        return(self)\n",
    "    \n",
    "    def swap_red(self):\n",
    "        #Check that you're not on red already and that the cooldown is 0.\n",
    "        if self._state[4] == 1 and self._state[7] == 0: #If swapping to red is possible and the swap is off-cooldown\n",
    "            self._state[3] = 0 #Set the current strategy to red\n",
    "\n",
    "            #Change ability to swap to other strategies\n",
    "            self._state[4] = 0 #Can't swap red\n",
    "            self._state[5] = 1 #Can swap green\n",
    "            self._state[6] = 1 #Can swap blue\n",
    "\n",
    "            self.reset_swap() #Set cooldown to 5\n",
    "            logging.info(\"Swapped red\")\n",
    "            \n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to swap red when it was on cooldown. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to swap red when it was on cooldown. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "        return(self)\n",
    "    \n",
    "    def swap_green(self):\n",
    "        if self._state[5] == 1 and self._state[7] == 0:\n",
    "            self._state[3] = 1\n",
    "\n",
    "            #Change ability to swap to other strategies\n",
    "            self._state[4] = 1\n",
    "            self._state[5] = 0\n",
    "            self._state[6] = 1\n",
    "\n",
    "            self.reset_swap()\n",
    "            logging.info(\"Swapped green\")\n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to swap green when it was on cooldown. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to swap green when it was on cooldown. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "        return(self)\n",
    "\n",
    "    def swap_blue(self):\n",
    "        if self._state[6] == 1 and self._state[7] == 0:\n",
    "            self._state[3] = 2\n",
    "\n",
    "            #Change ability to swap to other strategies\n",
    "            self._state[4] = 1\n",
    "            self._state[5] = 1\n",
    "            self._state[6] = 0\n",
    "\n",
    "            self.reset_swap()\n",
    "            logging.info(\"Swapped blue\")\n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to swap blue when it was on cooldown. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to swap blue when it was on cooldown. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "        return(self)\n",
    "\n",
    "    def use_sp_skill(self):\n",
    "        if self._state[10] == 1: #If the SP Skill is useable\n",
    "            self._state[10] = 0 #Set it to not being useable\n",
    "            self._state[8] = 0 #Set the SP gauge value to 0\n",
    "            self._state[2] += self._state[11] #Add the SP voltage to the score\n",
    "            self.reward = self._state[11] #Reward is the voltage\n",
    "            logging.info([\"Used SP skill for \", self._state[11], \"voltage. Current score: \",self._state[2],\". SP Gauge: \",self._state[8],\"/\",self._state[9],\". Note:\",self._state[0],\"/\",self._state[1]])\n",
    "        elif self.improper_use_counter <= self.max_mistakes:\n",
    "            self.improper_use_counter += 1\n",
    "            logging.info([\"Attempted to use SP skill while on cooldown. Mistakes made: \",self.improper_use_counter])\n",
    "            self.reward += -20000 #Punish it for trying to hit a note after the last one passed.\n",
    "        else:\n",
    "            logging.info(\"Attempted to use SP skill while on cooldown. Terminating play\")\n",
    "            self.reward += -20000\n",
    "            self._episode_ended = True\n",
    "        return(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = SongEnv()\n",
    "evaluation_py_env = SongEnv()\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "evaluation_env = tf_py_environment.TFPyEnvironment(evaluation_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 256  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 1  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1990000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "compute_avg_return(evaluation_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_env = tf_py_environment.TFPyEnvironment(SongEnvLogger())\n",
    "logger_env_py = SongEnvLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\tf_agents\\utils\\value_ops.py:89: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 70.92072296142578\n",
      "step = 400: loss = 49.652984619140625\n",
      "step = 600: loss = 178.75965881347656\n",
      "step = 800: loss = 34.42795181274414\n",
      "step = 1000: loss = 15.994593620300293\n",
      "step = 1000: Average Return = 800000.00\n",
      "step = 1200: loss = 481.079345703125\n",
      "step = 1400: loss = 59.13542556762695\n",
      "step = 1600: loss = 249.08526611328125\n",
      "step = 1800: loss = 44.43098449707031\n",
      "step = 2000: loss = 15.917967796325684\n",
      "step = 2000: Average Return = 4680000.00\n",
      "step = 2200: loss = 259.4224853515625\n",
      "step = 2400: loss = 173.61456298828125\n",
      "step = 2600: loss = 32.656768798828125\n",
      "step = 2800: loss = 59.935546875\n",
      "step = 3000: loss = 14.871188163757324\n",
      "step = 3000: Average Return = 48440000.00\n",
      "step = 3200: loss = 248.63262939453125\n",
      "step = 3400: loss = 3.325835943222046\n",
      "step = 3600: loss = 55.85826873779297\n",
      "step = 3800: loss = 11.66942024230957\n",
      "step = 4000: loss = 15.560126304626465\n",
      "step = 4000: Average Return = 0.00\n",
      "step = 4200: loss = 46.814964294433594\n",
      "step = 4400: loss = 10.203657150268555\n",
      "step = 4600: loss = 2.786141872406006\n",
      "step = 4800: loss = 97.02122497558594\n",
      "step = 5000: loss = 91.23837280273438\n",
      "step = 5000: Average Return = 8360000.00\n",
      "step = 5200: loss = 27.364768981933594\n",
      "step = 5400: loss = 70.16981506347656\n",
      "step = 5600: loss = 154.87567138671875\n",
      "step = 5800: loss = 4.8688812255859375\n",
      "step = 6000: loss = 1.0812962055206299\n",
      "step = 6000: Average Return = 312000.00\n",
      "step = 6200: loss = 50.48979187011719\n",
      "step = 6400: loss = 76.2988052368164\n",
      "step = 6600: loss = 11.320833206176758\n",
      "step = 6800: loss = 24.361404418945312\n",
      "step = 7000: loss = 6.761752605438232\n",
      "step = 7000: Average Return = 48440000.00\n",
      "step = 7200: loss = 3.8047895431518555\n",
      "step = 7400: loss = 10.652353286743164\n",
      "step = 7600: loss = 11.611931800842285\n",
      "step = 7800: loss = 2.5450241565704346\n",
      "step = 8000: loss = 8.552043914794922\n",
      "step = 8000: Average Return = 48440000.00\n",
      "step = 8200: loss = 23.264619827270508\n",
      "step = 8400: loss = 0.7204780578613281\n",
      "step = 8600: loss = 10.777236938476562\n",
      "step = 8800: loss = 0.6421166062355042\n",
      "step = 9000: loss = 0.6359039545059204\n",
      "step = 9000: Average Return = 2200000.00\n",
      "step = 9200: loss = 0.32977724075317383\n",
      "step = 9400: loss = 0.2874904274940491\n",
      "step = 9600: loss = 3.768754243850708\n",
      "step = 9800: loss = 0.29368856549263\n",
      "step = 10000: loss = 7.051605701446533\n",
      "step = 10000: Average Return = 3144000.00\n",
      "step = 10200: loss = 0.31032904982566833\n",
      "step = 10400: loss = 0.28968605399131775\n",
      "step = 10600: loss = 0.2992159426212311\n",
      "step = 10800: loss = 7.649317741394043\n",
      "step = 11000: loss = 0.3637508153915405\n",
      "step = 11000: Average Return = 5632000.00\n",
      "step = 11200: loss = 0.2891266345977783\n",
      "step = 11400: loss = 2.344648838043213\n",
      "step = 11600: loss = 0.27200430631637573\n",
      "step = 11800: loss = 0.28469160199165344\n",
      "step = 12000: loss = 3.307689666748047\n",
      "step = 12000: Average Return = 4376000.00\n",
      "step = 12200: loss = 0.3473871052265167\n",
      "step = 12400: loss = 2.6492624282836914\n",
      "step = 12600: loss = 1.0233045816421509\n",
      "step = 12800: loss = 0.3108321726322174\n",
      "step = 13000: loss = 0.33879008889198303\n",
      "step = 13000: Average Return = 5040000.00\n",
      "step = 13200: loss = 0.23114660382270813\n",
      "step = 13400: loss = 0.32660824060440063\n",
      "step = 13600: loss = 0.3029041886329651\n",
      "step = 13800: loss = 0.2965361475944519\n",
      "step = 14000: loss = 0.28122732043266296\n",
      "step = 14000: Average Return = 6030000.00\n",
      "step = 14200: loss = 0.2633856236934662\n",
      "step = 14400: loss = 0.36126819252967834\n",
      "step = 14600: loss = 0.2652513384819031\n",
      "step = 14800: loss = 0.22077208757400513\n",
      "step = 15000: loss = 0.38074979186058044\n",
      "step = 15000: Average Return = 27536000.00\n",
      "step = 15200: loss = 0.2648719549179077\n",
      "step = 15400: loss = 0.30972740054130554\n",
      "step = 15600: loss = 0.23603615164756775\n",
      "step = 15800: loss = 0.23842453956604004\n",
      "step = 16000: loss = 0.24742591381072998\n",
      "step = 16000: Average Return = 18896000.00\n",
      "step = 16200: loss = 0.22522833943367004\n",
      "step = 16400: loss = 0.22648680210113525\n",
      "step = 16600: loss = 0.29767847061157227\n",
      "step = 16800: loss = 0.3260774314403534\n",
      "step = 17000: loss = 0.1878906935453415\n",
      "step = 17000: Average Return = 21232000.00\n",
      "step = 17200: loss = 0.2827049791812897\n",
      "step = 17400: loss = 0.26005852222442627\n",
      "step = 17600: loss = 0.3165844678878784\n",
      "step = 17800: loss = 0.21911728382110596\n",
      "step = 18000: loss = 0.2207164615392685\n",
      "step = 18000: Average Return = 3144000.00\n",
      "step = 18200: loss = 0.26514720916748047\n",
      "step = 18400: loss = 0.21179354190826416\n",
      "step = 18600: loss = 0.2831559479236603\n",
      "step = 18800: loss = 0.2035464644432068\n",
      "step = 19000: loss = 0.27379417419433594\n",
      "step = 19000: Average Return = 3144000.00\n",
      "step = 19200: loss = 0.2655067443847656\n",
      "step = 19400: loss = 0.27063170075416565\n",
      "step = 19600: loss = 0.24885563552379608\n",
      "step = 19800: loss = 0.2649155855178833\n",
      "step = 20000: loss = 0.183073028922081\n",
      "step = 20000: Average Return = 18712000.00\n",
      "step = 20200: loss = 0.2833169400691986\n",
      "step = 20400: loss = 0.2037373185157776\n",
      "step = 20600: loss = 0.16833855211734772\n",
      "step = 20800: loss = 0.2576883137226105\n",
      "step = 21000: loss = 0.2539983093738556\n",
      "step = 21000: Average Return = 1974000.00\n",
      "step = 21200: loss = 0.24181485176086426\n",
      "step = 21400: loss = 0.2651304006576538\n",
      "step = 21600: loss = 0.19183340668678284\n",
      "step = 21800: loss = 0.29838165640830994\n",
      "step = 22000: loss = 0.18937097489833832\n",
      "step = 22000: Average Return = 23888000.00\n",
      "step = 22200: loss = 0.17814810574054718\n",
      "step = 22400: loss = 0.27819469571113586\n",
      "step = 22600: loss = 0.2263668179512024\n",
      "step = 22800: loss = 0.2122906744480133\n",
      "step = 23000: loss = 0.23300804197788239\n",
      "step = 23000: Average Return = 20096000.00\n",
      "step = 23200: loss = 0.19197195768356323\n",
      "step = 23400: loss = 0.1820506453514099\n",
      "step = 23600: loss = 0.3077128231525421\n",
      "step = 23800: loss = 0.19423575699329376\n",
      "step = 24000: loss = 0.20348434150218964\n",
      "step = 24000: Average Return = 20776000.00\n",
      "step = 24200: loss = 0.20959772169589996\n",
      "step = 24400: loss = 0.20050160586833954\n",
      "step = 24600: loss = 0.13901814818382263\n",
      "step = 24800: loss = 0.18810640275478363\n",
      "step = 25000: loss = 0.1705249547958374\n",
      "step = 25000: Average Return = 19712000.00\n",
      "step = 25200: loss = 0.2348308265209198\n",
      "step = 25400: loss = 0.23966865241527557\n",
      "step = 25600: loss = 0.18234658241271973\n",
      "step = 25800: loss = 0.2118622064590454\n",
      "step = 26000: loss = 0.21845124661922455\n",
      "step = 26000: Average Return = 5384000.00\n",
      "step = 26200: loss = 0.16101016104221344\n",
      "step = 26400: loss = 0.19617435336112976\n",
      "step = 26600: loss = 0.19391672313213348\n",
      "step = 26800: loss = 0.1694691926240921\n",
      "step = 27000: loss = 0.280049204826355\n",
      "step = 27000: Average Return = 22048000.00\n",
      "step = 27200: loss = 0.1356334686279297\n",
      "step = 27400: loss = 0.20152702927589417\n",
      "step = 27600: loss = 0.17226234078407288\n",
      "step = 27800: loss = 0.19089524447917938\n",
      "step = 28000: loss = 0.19633568823337555\n",
      "step = 28000: Average Return = 21136000.00\n",
      "step = 28200: loss = 0.17578908801078796\n",
      "step = 28400: loss = 0.2018992304801941\n",
      "step = 28600: loss = 0.17618177831172943\n",
      "step = 28800: loss = 0.18940699100494385\n",
      "step = 29000: loss = 0.21057496964931488\n",
      "step = 29000: Average Return = 1492000.00\n",
      "step = 29200: loss = 0.18919119238853455\n",
      "step = 29400: loss = 0.1599685549736023\n",
      "step = 29600: loss = 0.17052167654037476\n",
      "step = 29800: loss = 0.1685599535703659\n",
      "step = 30000: loss = 0.13208705186843872\n",
      "step = 30000: Average Return = 22336000.00\n",
      "step = 30200: loss = 0.1841803938150406\n",
      "step = 30400: loss = 0.14728085696697235\n",
      "step = 30600: loss = 0.1796712428331375\n",
      "step = 30800: loss = 0.18883082270622253\n",
      "step = 31000: loss = 0.17011713981628418\n",
      "step = 31000: Average Return = 22008000.00\n",
      "step = 31200: loss = 0.18728633224964142\n",
      "step = 31400: loss = 0.14948298037052155\n",
      "step = 31600: loss = 0.14100760221481323\n",
      "step = 31800: loss = 0.15250249207019806\n",
      "step = 32000: loss = 0.14367567002773285\n",
      "step = 32000: Average Return = 13776000.00\n",
      "step = 32200: loss = 0.12728913128376007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 32400: loss = 0.1411244422197342\n",
      "step = 32600: loss = 0.1874723881483078\n",
      "step = 32800: loss = 0.14056751132011414\n",
      "step = 33000: loss = 0.13380932807922363\n",
      "step = 33000: Average Return = 19984000.00\n",
      "step = 33200: loss = 0.21524861454963684\n",
      "step = 33400: loss = 0.1759824901819229\n",
      "step = 33600: loss = 0.2665477693080902\n",
      "step = 33800: loss = 0.13912764191627502\n",
      "step = 34000: loss = 0.18265564739704132\n",
      "step = 34000: Average Return = 3396000.00\n",
      "step = 34200: loss = 0.13583147525787354\n",
      "step = 34400: loss = 0.1517665982246399\n",
      "step = 34600: loss = 0.1939086765050888\n",
      "step = 34800: loss = 0.18067599833011627\n",
      "step = 35000: loss = 0.1977173388004303\n",
      "step = 35000: Average Return = 22896000.00\n",
      "step = 35200: loss = 0.144259974360466\n",
      "step = 35400: loss = 0.2222655862569809\n",
      "step = 35600: loss = 0.16149458289146423\n",
      "step = 35800: loss = 0.1481257975101471\n",
      "step = 36000: loss = 0.18491336703300476\n",
      "step = 36000: Average Return = 18688000.00\n",
      "step = 36200: loss = 0.12046626210212708\n",
      "step = 36400: loss = 0.1298145055770874\n",
      "step = 36600: loss = 0.14006389677524567\n",
      "step = 36800: loss = 0.1959359347820282\n",
      "step = 37000: loss = 0.1317833513021469\n",
      "step = 37000: Average Return = 12752000.00\n",
      "step = 37200: loss = 0.1668674349784851\n",
      "step = 37400: loss = 0.14885707199573517\n",
      "step = 37600: loss = 0.16450299322605133\n",
      "step = 37800: loss = 0.21004152297973633\n",
      "step = 38000: loss = 0.1856786161661148\n",
      "step = 38000: Average Return = 19200000.00\n",
      "step = 38200: loss = 0.15989261865615845\n",
      "step = 38400: loss = 0.09691674262285233\n",
      "step = 38600: loss = 0.15638750791549683\n",
      "step = 38800: loss = 0.11819145083427429\n",
      "step = 39000: loss = 0.1616988629102707\n",
      "step = 39000: Average Return = 14704000.00\n",
      "step = 39200: loss = 0.09581078588962555\n",
      "step = 39400: loss = 0.11604936420917511\n",
      "step = 39600: loss = 0.18501651287078857\n",
      "step = 39800: loss = 0.18090589344501495\n",
      "step = 40000: loss = 0.15630266070365906\n",
      "step = 40000: Average Return = 11312000.00\n",
      "step = 40200: loss = 0.21913142502307892\n",
      "step = 40400: loss = 0.20392069220542908\n",
      "step = 40600: loss = 0.18211869895458221\n",
      "step = 40800: loss = 0.14462830126285553\n",
      "step = 41000: loss = 0.1663656234741211\n",
      "step = 41000: Average Return = 18712000.00\n",
      "step = 41200: loss = 0.16019681096076965\n",
      "step = 41400: loss = 0.11327116191387177\n",
      "step = 41600: loss = 0.12052950263023376\n",
      "step = 41800: loss = 0.18595106899738312\n",
      "step = 42000: loss = 0.1401137411594391\n",
      "step = 42000: Average Return = 19560000.00\n",
      "step = 42200: loss = 0.14729973673820496\n",
      "step = 42400: loss = 0.1588909924030304\n",
      "step = 42600: loss = 0.11281123757362366\n",
      "step = 42800: loss = 0.09659937024116516\n",
      "step = 43000: loss = 0.1889132708311081\n",
      "step = 43000: Average Return = 25208000.00\n",
      "step = 43200: loss = 0.16655749082565308\n",
      "step = 43400: loss = 0.1259826123714447\n",
      "step = 43600: loss = 0.17134346067905426\n",
      "step = 43800: loss = 0.13187743723392487\n",
      "step = 44000: loss = 0.13116145133972168\n",
      "step = 44000: Average Return = 986000.00\n",
      "step = 44200: loss = 0.17602495849132538\n",
      "step = 44400: loss = 0.15128493309020996\n",
      "step = 44600: loss = 0.13254742324352264\n",
      "step = 44800: loss = 0.21898943185806274\n",
      "step = 45000: loss = 0.13528494536876678\n",
      "step = 45000: Average Return = 20664000.00\n",
      "step = 45200: loss = 0.1356792449951172\n",
      "step = 45400: loss = 0.22083492577075958\n",
      "step = 45600: loss = 0.10522769391536713\n",
      "step = 45800: loss = 0.16341394186019897\n",
      "step = 46000: loss = 0.14519667625427246\n",
      "step = 46000: Average Return = 2556000.00\n",
      "step = 46200: loss = 0.16226382553577423\n",
      "step = 46400: loss = 0.1899917721748352\n",
      "step = 46600: loss = 0.12139179557561874\n",
      "step = 46800: loss = 0.1652609258890152\n",
      "step = 47000: loss = 0.12661543488502502\n",
      "step = 47000: Average Return = 20296000.00\n",
      "step = 47200: loss = 0.12451213598251343\n",
      "step = 47400: loss = 0.08544407784938812\n",
      "step = 47600: loss = 0.14896580576896667\n",
      "step = 47800: loss = 0.12259996682405472\n",
      "step = 48000: loss = 0.0940062552690506\n",
      "step = 48000: Average Return = 11288000.00\n",
      "step = 48200: loss = 0.18067215383052826\n",
      "step = 48400: loss = 0.16263335943222046\n",
      "step = 48600: loss = 0.17182818055152893\n",
      "step = 48800: loss = 0.18993738293647766\n",
      "step = 49000: loss = 0.14916782081127167\n",
      "step = 49000: Average Return = 25256000.00\n",
      "step = 49200: loss = 0.1189262866973877\n",
      "step = 49400: loss = 0.1477062702178955\n",
      "step = 49600: loss = 0.11699187755584717\n",
      "step = 49800: loss = 0.12053895741701126\n",
      "step = 50000: loss = 0.1455029547214508\n",
      "step = 50000: Average Return = 19456000.00\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(evaluation_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience)\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(logger_env, agent.policy, num_eval_episodes)\n",
    "        logging.info([\"Step num: \",step,\"Avg return: \",avg_return])\n",
    "        print('step = {0}: Average Return = {1:.2f}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Step')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
